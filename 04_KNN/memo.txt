모델
입력데이터로 출력결과를 예측(설명)할 수 있는 수학 관계식(함수)

키, 몸무게
y = 0.7 * X + 30

[모델의 종류]
1. 지도학습
: 정답이 있는 데이터를 학습해서 예측하는 모델
(1) 회귀
    출력값 : 연속형 숫자 (점수, 가격, 온도)
    대표 모델 : 선형회귀, 다중회귀, 다항회귀

(2) 분류 (그룹 예측)
    출력값 : 범주형 (합격/불합격, 스팸/정상, 개/고양이)
    대표 모델 : 로지스틱 회귀, 서포트 벡터머신(SVM), K-최근접 이웃(KNN)

2. 비지도 학습
: 정답없이 데이터 구조나 패턴을 찾는 패턴
    1. 장바구니 분석(연관분석)
    2. 군집분석 (거리측도)
    3. DBSCAN 알고리즘 (밀도 기반)
    대표 모델 : K-means

선형회귀:
단순 선형회귀 : 독립변수 1개
다중 선형회귀 : 독립변수 2개


# 분류 알고리즘

## 1. SVM 알고리즘
⚫ SVM 주요 개념
1) 결정경계(Decision Boundary): 두 클래스를 구분하는 기준선 (decision_function = 0)
2) 마진(Margin): 결정경계와 서포트벡터(Support Vector) 간의 거리
3) 서포트벡터(Support Vector): 마진 경계선 위에 위치한 데이터 포인트들
4) 초평면(Hyperplane): 결정경계를 일반화한 개념 (2D에서는 직선, 3D에서는 평면)

⚫ 혼동행렬 (confusion matrix)
                예측
        negative    positive
실제(N)     TN          FP
실제(P)     FN          TP
TN : True Negative
FP : False Positive
FN : False Negative
TP : True Positive

Accuracy (정확도) = (TP + TN) / (TP + TN + FP + FN)
→ 전체 예측 중 맞춘 비율

Precision (정밀도) = TP / (TP + FP)
→ “합격”이라고 예측한 것 중 실제 합격 비율

NPV (Negative Predictive Value, 0의 정밀도) = TN / (TN + FN)
→ “불합격”이라고 예측한 것 중 실제 불합격 비율

Recall TPR(재현율, 민감도) = TP / (TP + FN)
→ 실제 합격자 중 모델이 맞게 예측한 비율

FPR (False Positive Rate, 거짓긍정률) =
FP / (FP + TN)
→ 실제 불합격자 중 “합격”으로 잘못 예측한 비율

Specificity (특이도) = TN / (TN + FP)
→ 실제 불합격자 중 모델이 맞게 예측한 비율


F1-score = 2 × (Precision × Recall) / (Precision + Recall)
→ 정밀도와 재현율의 조화평균

🌈 classification_report(실제 데이터, 예측데이터)

⚫ ROC / AUC (모델 성능 평가 지표)
1. ROC (Receiver Operating Characteristic)
1) X축: FPR = FP / (FP + TN) → 거짓긍정률
2) Y축: TPR = TP / (TP + FN) → 재현율(민감도)
3) 임계값(threshold)을 바꾸며 TPR과 FPR 변화를 그린 곡선

2. AUC (Area Under Curve)
1) ROC 곡선 아래 면적
2) 값이 1에 가까울수록 좋은 모델

## 2. KNN 알고리즘
# K-최근접 이웃(KNN : K-Nearest Neighbors)
새로운 데이터가 들어왔을 때, 기존 데이터 중 가장 가까운 K개의 점(이웃) 을 찾아
그 중 가장 많은 클래스(혹은 평균값) 으로 새 데이터를 분류하는 알고리즘

## 엔트로피 (Entropy)
데이터의 불확실성을 수치로 표현한 값.
클래스가 섞일수록 엔트로피 값이 커지고, 한쪽으로 치우칠수록 작다.
결정트리에서 데이터 분할 기준(정보이득)을 계산할 때 사용된다.

공식:
Entropy = - Σ(p_i * log₂(p_i))
(p_i: 각 클래스의 비율)

정보이득(Information Gain) = 분할 전 엔트로피 - 분할 후 엔트로피


## KNN (K-Nearest Neighbors)
새로운 데이터를 분류할 때, 가장 가까운 K개의 이웃 데이터를 찾아
다수결(분류) 또는 평균(회귀)으로 결과를 예측하는 알고리즘.
“가까운 데이터는 유사한 특성을 가진다”는 원리에 기반함.

거리 계산식(유클리드 거리):
d(x, y) = √Σ(x_i - y_i)²
특징:
장점: 단순하고 직관적, 학습 과정이 거의 없음
단점: 데이터 많을수록 계산량 증가, 이상치에 민감
K값이 작으면 과적합, 크면 과소적합


## 로지스틱 회귀
: 시그모이드 함수는 로지스틱 회귀분석과 인공신경망 분석에서 활성화 함수로 활용되는 대표적인 함수 중 하나다.
시그모이드 함수는 로짓 함수와 역함수 관계이기 때문에 로짓함수를 통해 시그모이드 함수가 도출된다.

■ 로지스틱 회귀(Logistic regression)
1) 로지스틱 회귀는 종속변수가 범주형인 경우로 의학연구에 많이 사용
2) 종속변수와 독립변수간의 관계를 나타내어 예측 모델을 생성한다는 점에서는 선형회귀 방법과 동일
3) 독립변수(x)에 의해 종속 변수(y)의 범주로 분류한다는 측면은 '분류 분석' 방법으로 분류

* 독립변수(입력 변수)는 연속형 or 범주형
* 종속변수(출력 변수)는 범주형

# 범주(생존/사망)를 분류
# 특정 클래스에 속할 확률 (0~1 사이)
# ex) 생존 확률(0.7)/사망확률(0.3) => 1/0
# 선형결합 결과(직선)를 Sigmoid 함수에 넣어 0~1로 변환
# 방정식
# z = w1x1 + w2x2 + wnxn + b
# 임의의 w,b
# sigmoid함수로 예측 확률 계산 -> 손실계산
# 역전파 (가중치와 편향을 조정하면서 손실이 각 가중치에 얼마나 영향을 줬는지 계산)

시그모이드
: 시그모이드 함수는 로지스틱 회귀분석과 인공신경망 분석에서 활성화 함수로 활용되는 대표적인 함수 중 하나다.
시그모이드 함수는 로짓 함수와 역함수 관계이기 때문에 로짓함수를 통해 시그모이드 함수가 도출된다.

방정식
z = w1x1 + w2x2 + wnxn + b

y_test = [1 0 0 1 1]
y_pred = [1 1 0 0 1]
손실값 (손실함수)

| 단계 | 과정       | 핵심 역할                   |
| -- | -------- | ----------------------- |
| 1  | 데이터 준비   | 입력(X), 정답(Y) 준비         |
| 2  | 선형 결합    | z 계산                    |
| 3  | 시그모이드 적용 | 확률 변환 (ŷ 계산)            |
| 4  | 손실함수 계산  | 오차(예측 vs 실제) 계산         |
| 5  | 역전파      | 오차가 가중치에 미치는 영향 계산      |
| 6  | 경사하강법    | w, b 갱신                 |
| 7  | 반복 학습    | 손실이 최소화될 때까지 반복         |
| 8  | 예측       | 새로운 데이터에 대한 확률 및 클래스 출력 |

# 시그모이드 → 손실함수 → 역전파 → 경사하강법 이 순환이 반복되며 모델이 점점 정확해지는 구조